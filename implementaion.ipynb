{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxy5yksd2ky0a4KvVuEJGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanikasanikachaudhari071/BE_project/blob/main/implementaion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA5hWkIVWxYi"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow keras opencv-python mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Conv2D, MaxPool2D, GlobalAveragePooling2D,\n",
        "    Concatenate, Lambda, LayerNormalization, MultiHeadAttention, Dropout\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "# Our input is a 224x224 RGB image\n",
        "input_shape = (224, 224, 3)\n",
        "img_input = Input(shape=input_shape)"
      ],
      "metadata": {
        "id": "LdM-8p5wXESt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the pre-trained DenseNet121 model spatial vector\n",
        "base_model = DenseNet121(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_tensor=img_input,\n",
        "    pooling='avg'  # This adds a GlobalAveragePooling2D layer\n",
        ")\n",
        "\n",
        "# 2. Freeze the base model layers (for initial training)\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# 3. This is our \"spatial vector\"\n",
        "spatial_vector = base_model.output"
      ],
      "metadata": {
        "id": "K9HNr5MZXG_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DCT Layer\n",
        "# We apply DCT on each channel independently\n",
        "def dct_layer(x):\n",
        "    # Apply 2D DCT (Type-II)\n",
        "    # tf.signal.dct expects float32 or float64\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    return tf.signal.dct(x, type=2, norm='ortho')\n",
        "\n",
        "# Wrap it in a Lambda layer\n",
        "freq_stream = Lambda(dct_layer)(img_input)\n",
        "\n",
        "# 2. Simple CNN\n",
        "# The DCT output is still 224x224x3\n",
        "cnn = Conv2D(32, (3, 3), activation='relu', padding='same')(freq_stream)\n",
        "cnn = MaxPool2D((2, 2))(cnn)\n",
        "\n",
        "cnn = Conv2D(64, (3, 3), activation='relu', padding='same')(cnn)\n",
        "cnn = MaxPool2D((2, 2))(cnn)\n",
        "\n",
        "# 3. This is our \"freq vector\"\n",
        "# We use GlobalAveragePooling2D to get a flat vector\n",
        "freq_vector = GlobalAveragePooling2D()(cnn)"
      ],
      "metadata": {
        "id": "s4vPdNdjXPUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check vector shapes (for debugging)\n",
        "print(f\"Spatial vector shape: {spatial_vector.shape}\")\n",
        "print(f\"Frequency vector shape: {freq_vector.shape}\")\n",
        "\n",
        "# 2. Concatenate them\n",
        "fused_features = Concatenate()([spatial_vector, freq_vector])"
      ],
      "metadata": {
        "id": "fj5eqBLGXT4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Transformer Encoder block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    # Note: MultiHeadAttention expects sequence input. We reshape.\n",
        "    # We treat our flat vector as a \"sequence\" of 1 element.\n",
        "    # This might need adjustment based on the \"Cross-ViT\" paper.\n",
        "    # A common trick is to expand dims to create a \"sequence\"\n",
        "\n",
        "    # Reshape for MHA: (batch_size, sequence_length, features)\n",
        "    # Let's assume the fused_features shape is (batch_size, num_features)\n",
        "    # We add a sequence dimension: (batch_size, 1, num_features)\n",
        "    x_seq = tf.expand_dims(x, axis=1)\n",
        "\n",
        "    attention_output = MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x_seq, x_seq) # Self-attention\n",
        "\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "    # Skip connection\n",
        "    x = x + attention_output[:, 0, :] # Squeeze back from sequence\n",
        "\n",
        "    # Feed-Forward Part\n",
        "    ffn = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ffn = Dense(ff_dim, activation=\"relu\")(ffn)\n",
        "    ffn = Dropout(dropout)(ffn)\n",
        "    ffn = Dense(inputs.shape[-1])(ffn) # Project back to original feature dim\n",
        "\n",
        "    # Second skip connection\n",
        "    transformer_output = x + ffn\n",
        "    return transformer_output\n",
        "\n",
        "# Apply the transformer block\n",
        "transformer_output = transformer_encoder(\n",
        "    fused_features,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=512,\n",
        "    dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "LUI-nxsvXXmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification MLP Head\n",
        "x = Dense(128, activation='relu')(transformer_output)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# Output layer: 1 neuron with sigmoid for binary (real/fake) classification\n",
        "# 0 = real, 1 = fake\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=img_input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print a summary to check your work\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pqGZ7UpRXaGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}